---
layout:     post
title:      java基础随记
subtitle:     由Netty引发对Linux IO的观察
date:       2019-05-26
author:     longyi
header-img: img/post-bg-swift2.jpg
catalog: true
tags:
    - java基础
    
---
>过去对Netty的了解，源于有哥们开发游戏提到这个东西，后来自己也简要的去搜索的看了看，只知道是一个高性能的网络通信框架。最近开始梳理一下IO相关的东西，也就不得不提Netty，而提了Netty就要去挖一下IO处理模型。

IO的性能想要提升，个人觉得只有从两个方面入手：

- 数据传输的格式
- 数据处理的方式

从数据格式上看，我们可以采用压缩的算法、自定义的协议等方式影响数据传输的格式，这个是数据的表现形式。

从数据处理的方式来看，可以采用不同的处理模型，比如单线程、多线程、1：N的模型等等方式。但是这里面有一个限定，就是需要操作系统内核层面去支持，毕竟你在应用层的“空想社会主义”得不到底层内核层面的支持，也就仅仅停留在“空想”的层面。

### 重点概念 

**同步与异步**：这是针对**多任务的处理方式**而言的，面对多任务，同步是线程每个任务必须要one next one 来处理。而异步是可以处理任务的时候也可以处理其他任务。

**阻塞与非阻塞**：这是针对**任务的调用方**而言的。如果处理的时候，调用方不能做任何事情那就是阻塞。如果还可以做其它事情那么就是非阻塞。

所以一般来说，只有异步和非阻塞相结合才有意义。

在linux系统中一切皆为文件，硬盘、ssd、cd这些我们俗称为块文件，也叫block device块设备，比较有意思的是英文block既有块的意思，也有阻塞、妨碍的意思。通常意义上我们都认为io都是block的，但是在linux系统中，文件的read不算阻塞，只有网络socket的IO才算是block，这是怎么理解的？

>有一种理解认为，网络IO的时候linux是无法知道对方是否会发送数据过来，这具有不可预知的特性，所以只能等待。而对于磁盘文件的IO，哪怕有磁盘抖动、重新寻址，这些是可预知的。

>补充：
>作者：沈万马
>链接：https://www.zhihu.com/question/52989189/answer/132997437
>传统的硬盘概念没有提供轮询传输状态的接口。不像网络设备设计之初就考虑到延迟的大变动范围和不确定性，于是设计上就提供了带状态检测的异步操作方式；硬盘作为一个自始至终都是“写不完就不行”的设备，异步操作接口是在最近几年才真正做到硬件接口里的。因为一直以来硬盘都是唯一可靠的持久本地存储，需要写入的数据如果不能写入意味着灾难性的后果。所以即便是现代操作系统，也并不轻易允许磁盘IO的纯异步化。

所以后面我们谈的关于NIO、AIO、IO多路复用只是针对网络IO。

#### BIO 

 阻塞式IO，一般在read、write、conncet一类的系统调用时都会卡住，对于网络处理程序来说，一个线程accept请求会一直挂起，直到获取到数据以后才进行下一步处理，所以对于server端则需要能支持开多线程才能支撑大的链接请求，由此带来了以下2个问题：

- 一个请求对应一个线程，则需要大量的线程去支撑client端，但是单端的能力总是受限的，比如内存资源。
- 多线程的情况下的上下文switch，性能较低。
- 只要请求不释放，则服务端则始终维持等待的线程，哪怕client端没有发送数据。无法释放，浪费资源严重。

所以针对以上场景，提出了NIO。

#### NIO

在BIO模式下，read如果没有数据返回，则持续block住。

在NIO模式下，调用read如果没有数据返回，立即返回-1.并且errorNo=EAGAIN。

所以在NIO模式下，则需要不断的轮训线程状态，但是效率比BIO的模式要好一些，因为它可以在未准备好的情况下做其它事情。

不过由此也引入了另外的问题：

- 如果有大量文件描述符（FD fileDescription，类似于真实文件的引用）要等，则需要依次轮训，带来大量的上下文切换（read是系统调用，每次调用就要用户态和内核态的切换）
- 轮询的时间不好把握，设置过长再去轮询则程序响应速度慢，设置的过短，频繁切换消耗cpu资源；

所以针对BIO的改进型NIO，提出了IO复用的概念。

#### IO多路复用（IO Multiplexing）

IO多路复用，是一种操作系统提供的便利的通知机制----**注意是操作系统提供的**。程序注册一组待检测的socket（FD）给系统，表示检视这些fd是否有事件发生，有了就告诉程序处理。

NIO与IO复用是两个独立的概念，但是通常是结合在一起使用。因为BIO和IO复用的话，依然还是block的，那么操作系统提供的这个机制是没有意义的。

对于IO复用，常见的误解：

- 多个数据流共享socket。错：本质上是os提供了多个socket的监控机制，而不是socket共享；
- 多路复用总是NIO的，所以总是不block住。错：其实IO复用的关键系统调用（select、poll、epoll_wait)总是block的；
- 多路复用和NIO减少了IO次数。错：其实模型只是提供了一种处理数据的方式，并不会因此而增加或者减少IO次数。

##### 操作系统提供的IO复用接口

>最早使用的是`select`和`poll`

###### select的样子：

    int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);

函数监听三个set，一个是读取事件readfds，一个是写事件writefds，一个是异常事件exceptfds。

常用的调用的伪代码如下：

	struct timeval tv = {.tv_sec = 1, .tv_usec = 0};

	ssize_t nbytes;
	while(1) {
    FD_ZERO(&read_fds);
    setnonblocking(fd1);
    setnonblocking(fd2);
    FD_SET(fd1, &read_fds);
    FD_SET(fd2, &read_fds);
    // 把要监听的fd拼到一个数组里，而且每次循环都得重来一次...
    if (select(FD_SETSIZE, &read_fds, NULL, NULL, &tv) < 0) { // block住，直到有事件到达
        perror("select出错了");
        exit(EXIT_FAILURE);
    }
    for (int i = 0; i < FD_SETSIZE; i++) {
        if (FD_ISSET(i, &read_fds)) {
            /* 检测到第[i]个读取fd已经收到了，这里假设buf总是大于到达的数据，所以可以一次read完 */
            if ((nbytes = read(i, buf, sizeof(buf))) >= 0) {
                process_data(nbytes, buf);
            } else {
                perror("读取出错了");
                exit(EXIT_FAILURE);
            }
        }
    }
	}

伪代码说明：
   构造fd读数组（为了简化，不构造写、异常事件的fd数组），之用调用select监听read_fds中的socket的读取时间。调用select之后，程序会block住，直到发生如下事件才返回：

	- 监听的socket有事件发生
	- 超时了，达到tv定义的1秒钟
 
之后需要遍历所有read_fds,检查是哪个fd有事件发生（FD_ISSET返回 true）。如果是，则说明有数据到达可以读取fd，进行数据处理。


select的缺陷：

- 函数支持的最大fd数组大小为1024，高并发服务明显达不到要求；
- select以后需要遍历fd，确定事件的fd，比较低效；
- fd数组按照监听事件分为读、写、异常，每次调用select要重新构造（因为select会改变他们的状态）
- select是无状态的，每次调用select，内核要重新检查一遍所有被注册的fd的状态。

###### poll的样子

poll与select类似：

	int poll(struct pollfd *fds, nfds_t nfds, int timeout);

poll的翻译为“轮询”。

>本质上，select与poll都是轮询。

poll优化了需要构造多个数组的问题，而且没有1024个数的限制。但是select的问题依然没有解决

- 依然是无状态的；
- 应用依然无法很方便的获取到发生事件的fd，还是需要遍历fd

目前来看，高性能的web服务器都**不会**使用select和poll。他们俩存在的意义仅仅是“兼容性”，因为很多操作系统都实现了这两个系统调用。

###### linux系统下的epoll（可以理解为增强型poll）

epoll是linux系统下io多路复用的一种实现，也是服务端高性能io的最佳实现。

与select、poll不同的是，使用epoll之前，需要先在内核态创建data表

	int epfd = epoll_create(10);

epoll_create在内核层创建了一个数据表，接口会返回一个“epoll的文件描述符”指向这个表。注意，接口参数是一个表达要监听事件列表的长度的数值。但不用太在意，因为epoll内部随后会根据事件注册和事件注销动态调整epoll中表格的大小。

为什么epoll要创建一个用文件描述符来指向的表呢？这里有两个好处：

- epoll是有状态的，不像select和poll那样每次都要重新传入所有要监听的fd，这避免了很多无谓的数据复制。epoll的数据是用接口epoll_ctl来管理的（增、删、改）。

- epoll文件描述符在进程被fork时，子进程是可以继承的。这可以给对多进程共享一份epoll数据，实现**并行**监听网络请求带来便利。


epoll在内核态的初始化fd个数后，就可以使用epoll_ctl来管理事件：

	int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

其中epfd就是上方创建的epfd，参数op则为操作类型：

- EPOLL_CTL_ADD 注册一个事件
- EPOLL_CTL_DEL 取消一个事件的注册
- EPOLL_CTL_MOD 修改一个事件的注册

第三个参数是要操作的fd，这里必须是支持NIO的fd（比如socket）

第四个参数是一个epoll_event的类型的数据，表达了注册的事件的具体信息。

	typedef union epoll_data {
	    void    *ptr;
	    int      fd;
	    uint32_t u32;
	    uint64_t u64;
	} epoll_data_t;
	
	struct epoll_event {
	    uint32_t     events;    /* Epoll events */
	    epoll_data_t data;      /* User data variable */
	};

定义了联合体epoll_data 的这样的类型为epoll_data_t类型，这个类型为epoll_event的成员变量。

比方说，想关注一个fd1的读取事件事件，并采用边缘触发，大概要这么写：

	struct epoll_data ev;
	ev.events = EPOLLIN | EPOLLET; // EPOLLIN表示读事件；EPOLLET表示边缘触发
	ev.data.fd = fd1;

所以通过epoll_ctl可以灵活修改、增加、删除fd。

第三步，在个数定义后，初始化好fd，则可以使用epoll_wait来等待事件发生

	int epoll_wait(int epfd, struct epoll_event *evlist, int maxevents, int timeout);

这一块是block的。只有当注册的事件至少有一个发生，或者timeout达到时，该调用才会返回。这与select和poll几乎一致。但不一样的地方是evlist，它是epoll_wait的返回数组，**里面只包含那些被触发的事件对应的fd**，而不是像select和poll那样返回所有注册的fd。

完整的伪代码：

    #define MAX_EVENTS 10
    struct epoll_event ev, events[MAX_EVENTS];
    int nfds, epfd, fd1, fd2;
    
    // 假设这里有两个socket，fd1和fd2，被初始化好。
    // 设置为non blocking
    setnonblocking(fd1);
    setnonblocking(fd2);
    
    // 创建epoll
    epfd = epoll_create(MAX_EVENTS);
    if (epollfd == -1) {
    perror("epoll_create1");
    exit(EXIT_FAILURE);
    }
    
    //注册事件
    ev.events = EPOLLIN | EPOLLET;
    ev.data.fd = fd1;
    if (epoll_ctl(epollfd, EPOLL_CTL_ADD, fd1, &ev) == -1) {
    perror("epoll_ctl: error register fd1");
    exit(EXIT_FAILURE);
    }
    if (epoll_ctl(epollfd, EPOLL_CTL_ADD, fd2, &ev) == -1) {
    perror("epoll_ctl: error register fd2");
    exit(EXIT_FAILURE);
    }
    
    // 监听事件
    for (;;) {
    nfds = epoll_wait(epdf, events, MAX_EVENTS, -1);
    if (nfds == -1) {
    perror("epoll_wait");
    exit(EXIT_FAILURE);
    }
    
    for (n = 0; n < nfds; ++n) { // 处理所有发生IO事件的fd
    process_event(events[n].data.fd);
    // 如果有必要，可以利用epoll_ctl继续对本fd注册下一次监听，然后重新epoll_wait
    }
    }
    
 按照手册中的说明：[epoll使用说明](http://man7.org/linux/man-pages/man7/epoll.7.html)

The following system calls are provided to create and manage an epoll
       instance:

       *  epoll_create(2) creates a new epoll instance and returns a file
          descriptor referring to that instance.  (The more recent
          epoll_create1(2) extends the functionality of epoll_create(2).)

       *  Interest in particular file descriptors is then registered via
          epoll_ctl(2), which adds items to the interest list of the epoll
          instance.

       *  epoll_wait(2) waits for I/O events, blocking the calling thread if
          no events are currently available.  (This system call can be
          thought of as fetching items from the ready list of the epoll
          instance.)  

所有的基于IO多路复用的代码都会遵循这样的写法：注册——监听事件——处理——再注册，无限循环下去。

通过以上的比较，可以得出epoll的优势：

- epoll将监听的fd传入到内核，避免了每次wait的copy；

- 事件发生时，epoll只返回有状态变更的fd list。（所以当发生事件的fd数量与注册事件个数一致时，epoll与select、poll相比无太多优势）

epoll除了以上性能优势以外，支持水平触发和边沿触发。

###### 水平触发 和边沿触发 

这个有点类似于我们在数字电路中提到的边沿和水平触发的含义，用数字电路中的概念去理解相信很容易的。

水平触发：代表的是一种状态，状态都有一个duration的概念。

边沿触发：代表的是一个事件。只关心是否有新的事件发生。

那么为什么需要边沿触发呢？业务场景是什么？一般情况下，若我们不关注fd的持有状态，而关注是否有**关注的事件发生以及对应的自定义处理方法。** 比如读取一个http请求，可以只读取部分数据再决定是否要继续读，而不是次次都要被“有数据”的状态烦扰。程序开发者可以根据业务要求，开发自定义的处理逻辑。

#### 总结 

综上，我们再来思考“block”的含义，其实不管是什么类型的IO都会有block的过程，可以理解为block就是一种wait的process，即使是NIO，也会有一个轮询的wait过程（主动去wait的过程），对于IO的多路复用也是有select、epoll_wait这样的阻塞型接口。所以，任何IO从本质上看都有一个阻塞的过程。


现在经常听到一个时尚名词叫做“zero copy",我当年知道这个名词的时候还是学习nginx的时候，记得当时设置一个参数sendfile on 就有这个效用。但是当时并未深入到linux 文件系统的层次去看待零拷贝的问题，只是简单的认为节约了内核态-用户态的数据copy次数而已。最近看了一个人在讲netty的时候，感觉里面有很多内容违背了操作系统基本的原理，个人认为零copy不可能是真正意义的零copy，只能说是减少了数据的cp而已。

 
